{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Cloning\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[?25h\u001b[?25lching:\u001b[22m\u001b[39m [========================================>]  100.0 %.0 %                                    ]  9.0 %                                ]  17.6 %\u001b[36m\u001b[1mFetching:\u001b[22m\u001b[39m [=================>                       ]  41.6 %]  81.2 %]  95.9 %"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaComputing/JuliaAcademyData.jl`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m registry at `C:\\Users\\power\\.julia\\registries\\General`\n",
      "┌ Warning: Some registries failed to update:\n",
      "│     — `C:\\Users\\power\\.julia\\registries\\General` — registry dirty\n",
      "└ @ Pkg.Types D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.4\\Pkg\\src\\Types.jl:1122\n",
      "\u001b[32m\u001b[1m  Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Rmath_jll ──── v0.2.2+0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OpenBLAS_jll ─ v0.3.9+4\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `D:\\git\\advection-diffusion-catalysis\\Project.toml`\n",
      " \u001b[90m [18b7da76]\u001b[39m\u001b[92m + JuliaAcademyData v0.1.0 #master (https://github.com/JuliaComputing/JuliaAcademyData.jl)\u001b[39m\n",
      " \u001b[90m [4536629a]\u001b[39m\u001b[95m ↓ OpenBLAS_jll v0.3.9+5 ⇒ v0.3.9+4\u001b[39m\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `D:\\git\\advection-diffusion-catalysis\\Manifest.toml`\n",
      " \u001b[90m [18b7da76]\u001b[39m\u001b[92m + JuliaAcademyData v0.1.0 #master (https://github.com/JuliaComputing/JuliaAcademyData.jl)\u001b[39m\n",
      " \u001b[90m [4536629a]\u001b[39m\u001b[95m ↓ OpenBLAS_jll v0.3.9+5 ⇒ v0.3.9+4\u001b[39m\n",
      " \u001b[90m [458c3c95]\u001b[39m\u001b[95m ↓ OpenSSL_jll v1.1.1+4 ⇒ v1.1.1+2\u001b[39m\n",
      " \u001b[90m [f50d1b31]\u001b[39m\u001b[95m ↓ Rmath_jll v0.2.2+1 ⇒ v0.2.2+0\u001b[39m\n",
      " \u001b[90m [83775a58]\u001b[39m\u001b[95m ↓ Zlib_jll v1.2.11+14 ⇒ v1.2.11+10\u001b[39m\n",
      "┌ Info: Precompiling JuliaAcademyData [18b7da76-0988-5e3b-acac-6290be3a708f]\n",
      "└ @ Base loading.jl:1260\n",
      "\u001b[32m\u001b[1m Activating\u001b[22m\u001b[39m environment at `C:\\Users\\power\\.julia\\packages\\JuliaAcademyData\\1to3l\\courses\\Parallel_Computing\\Project.toml`\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Arpack ──────────── v0.3.2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DiffRules ───────── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Parsers ─────────── v0.3.10\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m GPUArrays ───────── v2.0.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m IRTools ─────────── v0.3.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ForwardDiff ─────── v0.10.9\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Contour ─────────── v0.5.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CuArrays ────────── v1.6.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m URIParser ───────── v0.4.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CEnum ───────────── v0.2.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ArrayInterface ──── v2.2.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZipFile ─────────── v0.8.4\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DataStructures ──── v0.17.9\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m OffsetArrays ────── v1.0.2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m LLVM ────────────── v1.3.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Optim ───────────── v0.20.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ZygoteRules ─────── v0.2.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FixedPointNumbers ─ v0.6.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CodecZlib ───────── v0.6.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Conda ───────────── v1.3.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FFTW ────────────── v1.1.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Compose ─────────── v0.8.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Flux ────────────── v0.10.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Distributions ───── v0.22.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Rmath ───────────── v0.6.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m TimerOutputs ────── v0.5.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m NNlib ───────────── v0.6.4\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m SpecialFunctions ── v0.8.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m AbstractTrees ───── v0.3.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CUDAdrv ─────────── v5.0.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m WoodburyMatrices ── v0.5.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m ColorTypes ──────── v0.8.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Gadfly ──────────── v1.0.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m NLSolversBase ───── v7.5.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Adapt ───────────── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m FillArrays ──────── v0.8.4\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StatsFuns ───────── v0.9.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m StatsBase ───────── v0.32.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Calculus ────────── v0.5.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Interpolations ──── v0.12.7\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Zygote ──────────── v0.4.6\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Juno ────────────── v0.7.2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Requires ────────── v0.5.2\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CUDAnative ──────── v2.7.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m MacroTools ──────── v0.5.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DiffEqDiffTools ─── v1.6.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Ratios ──────────── v0.3.1\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m DistributedArrays ─ v0.6.4\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BenchmarkTools ──── v0.4.3\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m CUDAapi ─────────── v2.1.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Loess ───────────── v0.5.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m PDMats ──────────── v0.9.11\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m BinDeps ─────────── v1.0.0\n",
      "\u001b[32m\u001b[1m  Installed\u001b[22m\u001b[39m Colors ──────────── v0.9.6\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Arpack ──────────→ `C:\\Users\\power\\.julia\\packages\\Arpack\\zCmTA\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m ZipFile ─────────→ `C:\\Users\\power\\.julia\\packages\\ZipFile\\DW0Qr\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m CodecZlib ───────→ `C:\\Users\\power\\.julia\\packages\\CodecZlib\\5t9zO\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Conda ───────────→ `C:\\Users\\power\\.julia\\packages\\Conda\\kLXeC\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m FFTW ────────────→ `C:\\Users\\power\\.julia\\packages\\FFTW\\loJ3F\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m Rmath ───────────→ `C:\\Users\\power\\.julia\\packages\\Rmath\\BoBag\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m NNlib ───────────→ `C:\\Users\\power\\.julia\\packages\\NNlib\\3krvM\\deps\\build.log`\n",
      "\u001b[32m\u001b[1m   Building\u001b[22m\u001b[39m SpecialFunctions → `C:\\Users\\power\\.julia\\packages\\SpecialFunctions\\ne2iw\\deps\\build.log`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(Pkg.PackageSpec(url=\"https://github.com/JuliaComputing/JuliaAcademyData.jl\"))\n",
    "using JuliaAcademyData; activate(\"Parallel_Computing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading\n",
    "\n",
    "Now we're finally ready to start talking about running things on multiple\n",
    "processors! Most computers (even cell phones) these days have multiple cores\n",
    "or processors — so the obvious place to start working with parallelism is\n",
    "making use of those from within our Julia process.\n",
    "\n",
    "The first challenge, though, is knowing precisely how many \"processors\" you have.\n",
    "\"Processors\" is in scare quotes because, well, it's complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.4.1\n",
      "Commit 381693d3df* (2020-04-14 17:20 UTC)\n",
      "Platform Info:\n",
      "  OS: Windows (x86_64-w64-mingw32)\n",
      "      Microsoft Windows [Version 10.0.17763.1282]\n",
      "  CPU: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz: \n",
      "              speed         user         nice          sys         idle          irq\n",
      "       #1  2808 MHz   39293000            0     48329625    586279062      7043671  ticks\n",
      "       #2  2808 MHz   31898281            0     40598546    601404671      1094843  ticks\n",
      "       #3  2808 MHz   42061671            0     48372359    583467468       976515  ticks\n",
      "       #4  2808 MHz   24231234            0     32865046    616805218       505687  ticks\n",
      "       #5  2808 MHz   39244390            0     45691250    588965843       990750  ticks\n",
      "       #6  2808 MHz   22309328            0     26099796    625492375       973109  ticks\n",
      "       #7  2808 MHz   44514593            0     52116109    577270781      1082750  ticks\n",
      "       #8  2808 MHz   28671750            0     30079125    615150609       509562  ticks\n",
      "       \n",
      "  Memory: 15.918224334716797 GB (9642.94921875 MB free)\n",
      "  Uptime: 2.056257e6 sec\n",
      "  Load Avg:  0.0  0.0  0.0\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-8.0.1 (ORCJIT, skylake)\n",
      "Environment:\n",
      "  JULIA_NUM_THREADS = 4\n",
      "  HOMEDRIVE = C:\n",
      "  HOMEPATH = \\Users\\power\n",
      "  MEMU_PATH = C:\\Program Files (x86)\\Microvirt\n",
      "  PATH = C:\\Users\\power\\anaconda3;C:\\Users\\power\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\power\\anaconda3\\Library\\usr\\bin;C:\\Users\\power\\anaconda3\\Library\\bin;C:\\Users\\power\\anaconda3\\Scripts;C:\\Users\\power\\anaconda3\\bin;C:\\Users\\power\\anaconda3\\condabin;C:\\SIMULIA\\Commands;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\MATLAB\\R2019b\\runtime\\win64;C:\\Program Files\\MATLAB\\R2019b\\bin;C:\\cygwin\\bin;C:\\Program Files\\Git\\cmd;C:\\Program Files\\PuTTY;C:\\Users\\power\\anaconda3;C:\\Users\\power\\anaconda3\\python;C:\\Users\\power\\AppData\\Local\\Programs\\Python\\Python38;C:\\Users\\power\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\power\\AppData\\Roaming\\Dashlane\\6.1937.0.23352\\bin\\Firefox_Extension\\{442718d9-475e-452a-b3e1-fb1ee16b8e9f}\\components;C:\\Users\\power\\AppData\\Roaming\\Dashlane\\6.1937.0.23352\\ucrt;C:\\Users\\power\\AppData\\Roaming\\Dashlane\\6.1937.0.23352\\bin\\Qt;C:\\Users\\power\\AppData\\Roaming\\Dashlane\\6.1937.0.23352\\ucrt;C:\\Users\\power\\AppData\\Roaming\\Dashlane\\6.1937.0.23352\\bin\\Ssl;C:\\Users\\power\\AppData\\Local\\atom\\bin\n",
      "  PATHEXT = .COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n",
      "  PSMODULEPATH = C:\\Program Files\\WindowsPowerShell\\Modules;C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules\n",
      "  PYTHON_HOME = C:\\Users\\power\\AppData\\Roaming\\Microsoft\\Windows\n"
     ]
    }
   ],
   "source": [
    "versioninfo(verbose = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: proc not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: proc not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[4]:1"
     ]
    }
   ],
   "source": [
    ";cat /proc/cpuinfo # on Linux machines\n",
    "\n",
    "using Hwloc\n",
    "Hwloc.num_physical_cores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What your computer reports as the number of processors might not be the same\n",
    "as the total number of \"cores\". While sometimes virtual processors can add\n",
    "performance, parallelizing a typical numerical computation over these virtual\n",
    "processors will lead to significantly worse performance because they still\n",
    "have to share much of the nuts and bolts of the computation hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is somewhat multithreaded by default! BLAS calls (like matrix multiplication) are\n",
    "already threaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling BenchmarkTools [6e4b80f9-dd63-53aa-95a3-0cdb28fa8baf]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  149.396 ms (2 allocations: 30.52 MiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "A = rand(2000, 2000);\n",
    "B = rand(2000, 2000);\n",
    "@btime $A*$B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is — by default — already using all your CPU cores! You can see the effect\n",
    "by changing the number of threads (which BLAS supports doing dynamically):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  608.303 ms (2 allocations: 30.52 MiB)\n",
      "  204.050 ms (2 allocations: 30.52 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000×2000 Array{Float64,2}:\n",
       " 509.912  503.427  501.908  513.273  …  490.86   498.525  494.225  507.693\n",
       " 501.136  498.555  499.968  508.996     500.122  498.098  496.337  504.27\n",
       " 495.818  488.882  485.845  502.575     484.564  490.635  496.939  495.05\n",
       " 497.03   495.139  488.947  496.814     484.299  499.739  490.712  492.541\n",
       " 499.945  499.407  493.813  500.632     494.136  497.444  486.145  502.95\n",
       " 504.352  491.204  496.454  506.223  …  492.146  500.044  488.972  505.969\n",
       " 498.971  491.893  499.257  503.379     501.397  495.666  500.801  504.601\n",
       " 501.451  497.333  497.41   513.119     498.956  496.886  484.618  506.209\n",
       " 499.943  494.25   494.118  502.779     490.888  495.715  494.887  498.946\n",
       " 487.291  486.093  484.13   498.878     477.204  490.016  482.666  489.88\n",
       " 493.762  478.917  488.792  496.098  …  475.703  494.083  481.263  490.51\n",
       " 492.126  481.877  487.491  497.101     491.313  487.526  482.218  492.872\n",
       " 506.055  496.734  487.808  509.371     490.563  493.443  491.733  496.771\n",
       "   ⋮                                 ⋱                             \n",
       " 505.229  500.478  493.026  512.761     496.127  500.392  496.154  501.882\n",
       " 509.684  505.594  498.966  514.498     500.859  504.968  497.377  501.377\n",
       " 485.083  494.305  486.723  499.831  …  479.842  492.227  486.431  495.497\n",
       " 509.711  495.49   499.901  508.034     496.111  507.003  502.024  512.241\n",
       " 497.745  492.5    498.766  510.2       490.424  501.28   494.638  504.292\n",
       " 501.74   502.997  497.885  504.492     491.31   505.613  486.419  499.321\n",
       " 499.074  490.951  494.927  507.353     494.794  499.643  490.134  501.02\n",
       " 498.052  489.876  488.504  500.498  …  487.643  492.697  489.008  503.146\n",
       " 502.861  491.823  500.469  498.918     491.523  497.609  485.112  495.478\n",
       " 498.576  493.338  495.336  505.056     493.36   496.511  497.529  492.498\n",
       " 492.446  491.433  489.365  493.37      489.455  486.616  481.127  497.252\n",
       " 506.777  492.797  499.843  509.131     493.83   502.269  491.493  501.412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "BLAS.set_num_threads(1)\n",
    "@btime $A*$B\n",
    "BLAS.set_num_threads(4)\n",
    "@btime $A*$B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it look like to implement your _own_ threaded algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithreading support is marked as \"experimental\" for Julia 1.0 and is\n",
    "pending a big revamp for Julia version 1.2 or 1.3. The core tenets will be\n",
    "the same, but it should become much easier to use efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using .Threads\n",
    "\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia currently needs to start up knowing that it has threading support enabled.\n",
    "\n",
    "You do that with a environment variable. To get four threads, start Julia with:\n",
    "\n",
    "```\n",
    "JULIA_NUM_THREADS=4 julia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On JuliaBox, this is a challenge — we don't have access to the launching process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";env JULIA_NUM_THREADS=4 julia -E 'using .Threads; nthreads()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threadid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're currently on thread 1. Of course a loop like this will\n",
    "just set the first element to one a number of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Array{Union{Int,Missing}}(missing, nthreads())\n",
    "for i in 1:nthreads()\n",
    "    A[threadid()] = threadid()\n",
    "end\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we prefix it with `@threads` then the loop body runs on all threads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threads for i in 1:nthreads()\n",
    "    A[threadid()] = threadid()\n",
    "end\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try implementing our first simple threaded algorithm — `sum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_sum1(A)\n",
    "    r = zero(eltype(A))\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds r += A[i]\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "A = rand(10_000_000)\n",
    "threaded_sum1(A)\n",
    "@time threaded_sum1(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(A)\n",
    "@time sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! What happened? Not only did we get the wrong answer, it was _slow_ to get it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_sum2(A)\n",
    "    r = Atomic{eltype(A)}(zero(eltype(A)))\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds atomic_add!(r, A[i])\n",
    "    end\n",
    "    return r[]\n",
    "end\n",
    "\n",
    "threaded_sum2(A)\n",
    "@time threaded_sum2(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now we got the correct answer (modulo some floating point associativity),\n",
    "but it's still slower than just doing the simple thing on 1 core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threaded_sum2(A) ≈ sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's still slow! Using atomics is much slower than just adding integers\n",
    "because we constantly have to go and check _which_ processor has the latest\n",
    "work! Also remember that each thread is running on its own processor — and\n",
    "that processor also supports SIMD!  Well, that is if it didn't need to worry\n",
    "about syncing up with the other processors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_sum3(A)\n",
    "    r = Atomic{eltype(A)}(zero(eltype(A)))\n",
    "    len, rem = divrem(length(A), nthreads())\n",
    "    @threads for t in 1:nthreads()\n",
    "        rₜ = zero(eltype(A))\n",
    "        @simd for i in (1:len) .+ (t-1)*len\n",
    "            @inbounds rₜ += A[i]\n",
    "        end\n",
    "        atomic_add!(r, rₜ)\n",
    "    end\n",
    "    # catch up any stragglers\n",
    "    result = r[]\n",
    "    @simd for i in length(A)-rem+1:length(A)\n",
    "        @inbounds result += A[i]\n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "threaded_sum3(A)\n",
    "@time threaded_sum3(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dang, that's complicated. There's also a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threaded_sum3(rand(10) .+ rand(10)im) # try an array of complex numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't there an easier way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = zeros(eltype(A), nthreads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threaded_sum4(A)\n",
    "    R = zeros(eltype(A), nthreads())\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds R[threadid()] += A[i]\n",
    "    end\n",
    "    r = zero(eltype(A))\n",
    "    # sum the partial results from each thread\n",
    "    for i in eachindex(R)\n",
    "        @inbounds r += R[i]\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "threaded_sum4(A)\n",
    "@time threaded_sum4(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sacrifices our ability to `@simd` so it's a little slower, but at least we don't need to worry\n",
    "about all those indices! And we also don't need to worry about atomics and\n",
    "can again support arrays of any elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threaded_sum4(rand(10) .+ rand(10)im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways from `threaded_sum`:\n",
    "\n",
    "* Beware shared state across threads — it may lead to wrong answers!\n",
    "    * Protect yourself by using atomics (or [locks/mutexes](https://docs.julialang.org/en/v1/base/multi-threading/#Synchronization-Primitives-1))\n",
    "    * Better yet: divide up the work manually such that the inner loops don't\n",
    "      share state. `@threads for i in 1:nthreads()` is a handy idiom.\n",
    "    * Alternatively, just use an array and only access a single thread's elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware of global state (even if it's not obvious!)\n",
    "\n",
    "Another class of algorithm that you may want to parallelize is a monte-carlo\n",
    "problem. Since each iteration is a new random draw, and since you're interested\n",
    "in looking at the aggregate result, this seems like it should lend itself to\n",
    "parallelism quite nicely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function serialpi(n)\n",
    "    inside = 0\n",
    "    for i in 1:n\n",
    "        x, y = rand(), rand()\n",
    "        inside += (x^2 + y^2 <= 1)\n",
    "    end\n",
    "    return 4 * inside / n\n",
    "end\n",
    "serialpi(1)\n",
    "@time serialpi(100_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using .Threads\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the techniques we learned above to make a fast threaded implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function threadedpi(n)\n",
    "    inside = zeros(Int, nthreads())\n",
    "    @threads for i in 1:n\n",
    "        x, y = rand(), rand()\n",
    "        @inbounds inside[threadid()] += (x^2 + y^2 <= 1)\n",
    "    end\n",
    "    return 4 * sum(inside) / n\n",
    "end\n",
    "threadedpi(100_000_000)\n",
    "@time threadedpi(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now why didn't that work?  It's slow! Let's look at the sequence of random\n",
    "numbers that we generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random\n",
    "Random.seed!(0)\n",
    "N = 20000\n",
    "Rserial = zeros(N)\n",
    "for i in 1:N\n",
    "    Rserial[i] = rand()\n",
    "end\n",
    "Rserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(0)\n",
    "Rthreaded = zeros(N)\n",
    "@threads for i in 1:N\n",
    "    Rthreaded[i] = rand()\n",
    "end\n",
    "Rthreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set(Rserial) == Set(Rthreaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexin(Rserial, Rthreaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, `rand()` isn't threadsafe! It's mutating (and reading) some global each\n",
    "time to figure out what to get next. This leads to slowdowns — and worse — it\n",
    "skews the generated distribution of random numbers since some are repeated!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const ThreadRNG = Vector{Random.MersenneTwister}(undef, nthreads())\n",
    "@threads for i in 1:nthreads()\n",
    "    ThreadRNG[Threads.threadid()] = Random.MersenneTwister()\n",
    "end\n",
    "function threadedpi2(n)\n",
    "    inside = zeros(Int, nthreads())\n",
    "    len, rem = divrem(n, nthreads())\n",
    "    rem == 0 || error(\"use a multiple of $(nthreads()), please!\")\n",
    "    @threads for i in 1:nthreads()\n",
    "        rng = ThreadRNG[threadid()]\n",
    "        v = 0\n",
    "        for j in 1:len\n",
    "            x, y = rand(rng), rand(rng)\n",
    "            v += (x^2 + y^2 <= 1)\n",
    "        end\n",
    "        inside[threadid()] = v\n",
    "    end\n",
    "    return 4 * sum(inside) / n\n",
    "end\n",
    "threadedpi2(10)\n",
    "@time threadedpi2(100_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, be careful about initializing many `MersenneTwister`s with\n",
    "different states. Better to use [`randjump`](https://docs.julialang.org/en/v1/manual/parallel-computing/#Side-effects-and-mutable-function-arguments-1) to skip ahead for a single state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware oversubscription\n",
    "\n",
    "Remember how BLAS is threaded by default? What happens if we try to `@threads`\n",
    "something that uses BLAS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ms = [rand(1000, 1000) for _ in 1:100]\n",
    "function serial_matmul(As)\n",
    "    first_idxs = zeros(length(As))\n",
    "    for i in eachindex(As)\n",
    "        @inbounds first_idxs[i] = (As[i]'*As[i])[1]\n",
    "    end\n",
    "    first_idxs\n",
    "end\n",
    "serial_matmul(Ms);\n",
    "@time serial_matmul(Ms);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "BLAS.set_num_threads(nthreads()) # Explicitly tell BLAS to use the same number of threads\n",
    "function threaded_matmul(As)\n",
    "    first_idxs = zeros(length(As))\n",
    "    @threads for i in eachindex(As)\n",
    "        @inbounds first_idxs[i] = (As[i]'*As[i])[1]\n",
    "    end\n",
    "    first_idxs\n",
    "end\n",
    "threaded_matmul(Ms)\n",
    "@time threaded_matmul(Ms);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLAS.set_num_threads(1)\n",
    "@time threaded_matmul(Ms);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time serial_matmul(Ms) # Again, now that BLAS has just 1 thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beware \"false sharing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the memory latency table?\n",
    "\n",
    "\n",
    "| System Event                   | Actual Latency | Scaled Latency |                          |\n",
    "| ------------------------------ | -------------- | -------------- | ------------------------ |\n",
    "| One CPU cycle                  |     0.4 ns     |     1 s        | ← work happens here     |\n",
    "| Level 1 cache access           |     0.9 ns     |     2 s        |                          |\n",
    "| Level 2 cache access           |     2.8 ns     |     7 s        |                          |\n",
    "| Level 3 cache access           |      28 ns     |     1 min      |                          |\n",
    "| Main memory access (DDR DIMM)  |    ~100 ns     |     4 min      | ← we have control here  |\n",
    "\n",
    "This is what a typical modern cpu looks like:\n",
    "\n",
    "![Intel Core i7](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/i7.jpg)\n",
    "\n",
    "Multiple cores on the same processor share the L3 cache, but do not share L1 and L2 caches! So what happens if we're accessing and mutating data from the same array across multiple cores?\n",
    "\n",
    "![Cache coherency](https://raw.githubusercontent.com/JuliaComputing/JuliaAcademyData.jl/master/courses/Parallel_Computing/images/false-sharing.gif)\n",
    "\n",
    "Unlike \"true\" sharing — which we saw above — false sharing will still return the correct answer! But it does so at the cost of performance. The cores recognize they don't have exclusive access to the cache line and so upon modification they alert all other cores to invalidate and re-fetch the data.\n",
    "\n",
    "```julia\n",
    "function threaded_sum4(A)\n",
    "    R = zeros(eltype(A), nthreads())\n",
    "    @threads for i in eachindex(A)\n",
    "        @inbounds R[threadid()] += A[i]\n",
    "    end\n",
    "    r = zero(eltype(A))\n",
    "    # sum the partial results from each thread\n",
    "    for i in eachindex(R)\n",
    "        @inbounds r += R[i]\n",
    "    end\n",
    "    return r\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further improvements coming here!\n",
    "\n",
    "PARTR — the threading improvement I discussed at the beginning aims to address\n",
    "this problem of having library functions implemented with `@threads` and then\n",
    "having callers call them with `@threads`. Uses a state-of-the-art work queue\n",
    "mechanism to make sure that all threads stay busy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading takeaways:\n",
    "\n",
    "* It's easy! Just start Julia with `JULIA_NUM_THREADS` and tack a `@threads` on your loop\n",
    "* Well, not so fast\n",
    "    * Be aware of your hardware to set `JULIA_NUM_THREADS` appropiately\n",
    "    * Beware shared state (for both performance and correctness)\n",
    "    * Beware global state (even if it's not obvious)\n",
    "    * Beware false sharing (if Julia/LLVM don't handle it for you)\n",
    "* We need to think carefully about how to design parallel algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
